<!-- <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<center>
<head>
	<title> A Neural Network for Detailed Human Depth Estimation from a Single Image </title>
	<link rel="stylesheet" type="text/css" media="screen" href="css/style.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />	
</head>


<body>

<div id="paper-title"> <h1> A Neural Network for Detailed Human Depth Estimation from a Single Image </h1> </div>

<div id="venue">
	ICCV  2019
</div>

<div id="author-list">  
	<a> Sicong Tang<sup>1</sup> </a>
	<a> Feitong Tan<sup>1</sup> </a>
	<a> Kelvin Cheng<sup>1</sup> </a>
	<a> Zhaoyang Li<sup>1</sup> </a>
	<a> Siyu Zhu<sup>2</sup> </a>
	<a> Ping Tan<sup>1</sup> </a>
</div>

<div id="author-affiliation">
	<span class="affiliation"> <sup>1</sup>Simon Fraser University </span>
	<span class="affiliation"> <sup>2</sup>Alibaba AI labs </span>
</div>


<div id="pipeline-part">
	<a class="imageLink" href="./pipeline.png"><img id="pipeline" src="./pipeline.png" alt="Pipeline" width="1100"></a>
</div>

<div id="abstract">
	<h2 class="caption" align="center"> Abstract </h2>
	<p class="paragraph" align="left">  
This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB
image. The result captures geometry details such as cloth
wrinkles, which are important in visualization applications.
To achieve this goal, we separate the depth map into a
smooth base shape and a residual detail shape and design
a network with two branches to regress them respectively.
We design a training strategy to ensure both base and detail
shapes can be faithfully learned by the corresponding
network branches. Furthermore, we introduce a novel network
layer to fuse a rough depth map and surface normals
to further improve the final result. Quantitative comparison
with fused ‘ground truth’ captured by real depth cameras
and qualitative examples on unconstrained Internet images
demonstrate the strength of the proposed method.
	</p>
</div>

<div id="package">
	<a id="Paper" href="https://arxiv.org/pdf/1910.01275.pdf"> Paper </a>
	<a id="Code" href="https://github.com/sfu-gruvi-3dv/deep_human">  Code </a>
	<a id="Slides" href="https://drive.google.com/file/d/1eQRvwxeMNkWkaX7I3DG4PVOlRiL5ZIne/view?usp=sharing">  Slides  </a>
</div>

<div id="video">
	<h2 class="caption">Demo Video </h2>
	<br/>
	<iframe width="560" height="315" src="https://www.youtube.com/embed/ulLpIYHcnCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<div id="acknowledgement">
	<h2 class="caption"> Acknowledgements </h2>
	<p class="paragraph" align="left"> 
	This work is supported by the NSERC Discovery grant 611664, Discovery Acceleration Supplements 611663, and a research gift from Alibaba AI labs.
	</p>
</div>

</body>
</center>
</html>
 -->


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-66428408-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-66428408-2');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>A Neural Network for Detailed Human Depth Estimation from a Single Image</title>
        <meta property="og:title" content="Deep Human" />
        <meta property="og:image" content="./pipeline.png" />
        <meta property="og:url" content="https://www.youtube.com/embed/ulLpIYHcnCo" />
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">A Neural Network for Detailed Human Depth Estimation from a Single Image</span>
    
    </center>

    <br><br>
      <table align=center width=800px>
       <tr>
         <td align=center width=100px>
         <center>
         <span style="font-size:20px">Sicong Tang<sup>1 *</sup></a></span>
         </center>
         </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px">Feitong Tan<sup>1 *</sup></a></span>
        </center>
        </td>

        <td align=center width=100px>
          <center>
          <span style="font-size:20px">Kelvin Cheng<sup>1</sup></a></span>
          </center>
        </td>
      <tr>
      </tr>
        <td align=center width=100px>
          <center>
          <span style="font-size:20px">Zhaoyang Li<sup>1</sup></a></span>
          </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px">Siyu Zhu<sup>2</sup></a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px">Ping Tan<sup>1</sup></a></span>
        </center>
        </td>
     </tr>
    </table>

    <table align=center width=800px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><br/>Simon Fraser University<sup>1</sup><br/>Alibaba AI Labs<sup>2</sup></span>
        </center>
        </td>
     </tr>
    </table>

    <br>
    <table align=center width=400px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px"><a href="https://arxiv.org/pdf/1910.01275.pdf">[Paper]</a></span>
       </center>
       </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://youtu.be/ulLpIYHcnCo">[Video]</a></span>
      </center>
      </td>
       
      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://github.com/sfu-gruvi-3dv/deep_human">[Code]</a></span>
      </center>
      </td>
      
   </tr>
  </table>

            <br>
            <br>
            <table align=center width=600px>
                <tr>
                    <td width=600px>
                      <center>
                          <a href="https://youtu.be/ulLpIYHcnCo"><img src = "./pipeline.png" height="300px"></img></href></a><br>
                    </center>
                    </td>
                </tr>
                    <td width=600px>
                    </td>
                </tr>
            </table>

            <br>
              <div style="max-width:90%; margin:0 auto">
                This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB
image. The result captures geometry details such as cloth
wrinkles, which are important in visualization applications.
To achieve this goal, we separate the depth map into a
smooth base shape and a residual detail shape and design
a network with two branches to regress them respectively.
We design a training strategy to ensure both base and detail
shapes can be faithfully learned by the corresponding
network branches. Furthermore, we introduce a novel network
layer to fuse a rough depth map and surface normals
to further improve the final result. Quantitative comparison
with fused ‘ground truth’ captured by real depth cameras
and qualitative examples on unconstrained Internet images
demonstrate the strength of the proposed method.<br><br>
              </div>
          <hr>
         <!-- <table align=center width=550px> -->
            <table align=center width=800>
             <center><h1>Paper</h1></center>
                <tr>
                  <td><a href="https://arxiv.org/pdf/1910.01275.pdf"><img style="height:200px" src="./demo.png"/></a></td>
                  <td><span style="font-size:14pt">Tang*, Tan*, Cheng*, Li, Zhu, Tan.<br><br>
                    A Neural Network for Detailed Human Depth Estimation from a Single Image. <br><br>
                    ICCV 2019.(Oral)<br><br>
                      <a href="https://arxiv.org/pdf/1910.01275.pdf">[pdf]</a> &nbsp; &nbsp;
                    <a href="./bibtex.txt">[Bibtex]</a>
                    </td>
              </tr>
            </table>
          <br>

                <hr>
                <center><h1>Paper Video</h1></center>
                <table align=center width=900px>
                    <tr>
                        <td width=600px>
                          <center>
                            <div class = "video">
                              <iframe width="720" height="405" src="https://www.youtube.com/embed/ulLpIYHcnCo" frameborder="0" allowfullscreen></iframe>
                           </div>
                        </center>
                        </td>
                    </tr>
                </table>
                <br>
                <hr>

        <br><br>
</body>
</html>

